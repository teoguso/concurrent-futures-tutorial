{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "from itertools import islice\n",
    "from google.cloud import storage, bigquery\n",
    "\n",
    "# Note: you'll need a functioning Google Cloud account to be able to use this notebook\n",
    "from secrets.secrets import PROJECT_ID, BUCKET_URI, GS_FOLDER, BQ_DATASET, BQ_TABLE\n",
    "from helpers import (\n",
    "    n_fibonacci, calc_lengths,\n",
    "    calc_length, print_stuff,\n",
    "    create_bigquery_client,\n",
    "    stream_data, bq_data_insert\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"notebook\")\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blobs():\n",
    "    \"\"\"Get blobs for files on storage\"\"\"\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.get_bucket(BUCKET_URI)\n",
    "    \n",
    "    blob_names = (\n",
    "        b.name for b in bucket.list_blobs(\n",
    "            prefix=\"data\",\n",
    "        ) if not b.name.endswith(\"/\")\n",
    "    )\n",
    "    for bname in blob_names:\n",
    "        yield bucket.blob(bname)\n",
    "\n",
    "def download_as_string(myblob):\n",
    "    return myblob.download_as_string().decode()\n",
    "\n",
    "def blobs_slice():\n",
    "    return islice(get_blobs(), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"col1\": 200006, \"col2\": 41799}\n",
      "{\"col1\": 200003, \"col2\": 41798}\n",
      "{\"col1\": 200006, \"col2\": 41798}\n",
      "{\"col1\": 200001, \"col2\": 41799}\n",
      "{\"col1\": 200004, \"col2\": 41798}\n",
      "{\"col1\": 200009, \"col2\": 41799}\n",
      "{\"col1\": 200002, \"col2\": 41799}\n",
      "{\"col1\": 200008, \"col2\": 41799}\n",
      "CPU times: user 4.29 s, sys: 80 ms, total: 4.37 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Sequential code\n",
    "seq_sizes = [\n",
    "    download_as_string(b) for b in blobs_slice()\n",
    "]\n",
    "seq_sizes = map(int, seq_sizes)\n",
    "fibonaccis = map(n_fibonacci, seq_sizes)\n",
    "sizes = map(lambda x: len(str(x)), fibonaccis)\n",
    "# number_size = calc_length(fibonacci)\n",
    "for a, b in zip(seq_sizes, sizes):\n",
    "    data = json.dumps(\n",
    "        dict(\n",
    "            col1=a,\n",
    "            col2=b,\n",
    "        )\n",
    "    )\n",
    "    print(data)\n",
    "    stream_data(data, BQ_DATASET, BQ_TABLE)\n",
    "# print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "upstream_mytable = partial(\n",
    "    stream_data,\n",
    "    dataset_name=BQ_DATASET,\n",
    "    table_name=BQ_TABLE,\n",
    ")\n",
    "\n",
    "def cpu_bound_work(input_number):\n",
    "    return len(str(n_fibonacci(int(input_number))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "CPU times: user 1.26 s, sys: 212 ms, total: 1.48 s\n",
      "Wall time: 8.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Concurrent code 1\n",
    "with ProcessPoolExecutor() as pp:\n",
    "    with ThreadPoolExecutor() as tp:\n",
    "        # This needs to stay in memory\n",
    "        seq_sizes = list(\n",
    "            tp.map(\n",
    "                download_as_string, blobs_slice()\n",
    "            )\n",
    "        )\n",
    "        sizes = pp.map(\n",
    "            cpu_bound_work, seq_sizes\n",
    "        )\n",
    "        data = [\n",
    "            json.dumps(\n",
    "                dict(\n",
    "                    col1=a,\n",
    "                    col2=b,\n",
    "                )\n",
    "            ) for a, b in zip(seq_sizes, sizes)\n",
    "        ]\n",
    "#         print(list(data))\n",
    "        stream = tp.map(\n",
    "            upstream_mytable, data\n",
    "        )\n",
    "        print(list(stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['200006', '200006', '200003', '200006', '200001', '200005', '200002', '200004', '200001', '200001', '200005', '200009', '200008', '200007', '200002', '200006'])\n",
      "CPU times: user 720 ms, sys: 176 ms, total: 896 ms\n",
      "Wall time: 7.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Concurrent code 2\n",
    "with ProcessPoolExecutor() as pp:\n",
    "    with ThreadPoolExecutor() as tp:\n",
    "        futures = [\n",
    "            tp.submit(\n",
    "                    download_as_string, b\n",
    "                ) for b in blobs_slice()\n",
    "        ]\n",
    "        length_futures = {\n",
    "            pp.submit(\n",
    "                cpu_bound_work, f.result()\n",
    "            ): f.result() for f in as_completed(futures)\n",
    "        }\n",
    "        for f in as_completed(length_futures):\n",
    "            data = json.dumps(\n",
    "                dict(\n",
    "                    col1=length_futures[f],\n",
    "                    col2=f.result(),\n",
    "                )\n",
    "            )\n",
    "#             print(data)\n",
    "            tp.submit(upstream_mytable, data)\n",
    "\n",
    "        print(length_futures.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
